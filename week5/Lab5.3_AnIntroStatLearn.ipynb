{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab on Cross-Validation is a python adaptation of p. 190-194 of \"Introduction to Statistical Learning\n",
    "with Applications in R\" by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. From\n",
    "<t>http://www.science.smith.edu/~jcrouser/SDS293/labs/</t>\n",
    "\n",
    "### 5.3.1 The Validation Set Approach\n",
    "\n",
    "The aim is to use the validation set approach in order to estimate the test error rates that result from fitting various linear models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                     #library for scientific computing\n",
    "import matplotlib.pyplot as plt        #library for plotting data\n",
    "import pandas as pd                    #library for data analysis\n",
    "import sklearn.linear_model as skl_lm  #library for machine (statistical) learning\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the data set <b>Auto</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 392 entries, 0 to 396\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   mpg           392 non-null    float64\n",
      " 1   cylinders     392 non-null    int64  \n",
      " 2   displacement  392 non-null    float64\n",
      " 3   horsepower    392 non-null    float64\n",
      " 4   weight        392 non-null    int64  \n",
      " 5   acceleration  392 non-null    float64\n",
      " 6   year          392 non-null    int64  \n",
      " 7   origin        392 non-null    int64  \n",
      " 8   name          392 non-null    object \n",
      "dtypes: float64(4), int64(4), object(1)\n",
      "memory usage: 30.6+ KB\n"
     ]
    }
   ],
   "source": [
    "thisdata = pd.read_csv('Auto.csv', na_values='?').dropna() #read csv, DROPping Non Available data\n",
    "thisdata.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate the training set:</b>\n",
    "Split the set of observations into two halves, randomly choosen (use function <tt>sample()</tt>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = thisdata.sample(196, random_state = 1)  #random_state = set a seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Generate the test set:</b>\n",
    "Take the rest of the 392 data points (use function <tt>isin</tt>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = thisdata[~thisdata.isin(train_set)].dropna(how = 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the variables <tt>horsepower</tt> and <tt>mpg</tt> in each set (use function <tt>values</tt> to plot only the values of the variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xtrain = train_set['horsepower']\n",
    "#Ytrain = train_set['mpg']\n",
    "#Xtest = test_set['horsepower']\n",
    "#Ytest = test_set['mpg']\n",
    "Xtrain = train_set['horsepower'].values\n",
    "Ytrain = train_set['mpg'].values\n",
    "Xtest = test_set['horsepower'].values\n",
    "Ytest = test_set['mpg'].values\n",
    "#print(Xtrain,'\\n',Ytrain,'\\n',Xtest,'\\n',Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give a new shape to the arrays <tt>horsepower</tt> without changing the values. Note: The \"-1\" in <tt>reshape(-1,1)</tt> means that we do not known the dimension and we want <tt>numpy</tt> to figure it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = train_set['horsepower'].values.reshape(-1,1) \n",
    "Ytrain = train_set['mpg'].values\n",
    "Xtest = test_set['horsepower'].values.reshape(-1,1)\n",
    "Ytest = test_set['mpg'].values\n",
    "#print(Xtrain,'\\n',Ytrain,'\\n',Xtest,'\\n',Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Linear regression</b>: \n",
    "We now fit a linear regression to the data to predict <tt>mpg</tt> (miles per gallon) from <tt>horsepower</tt> using only the observations corresponding to the training set. Note: we use here function <tt> LinearRegression()</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.15964606] 40.3338030434159\n"
     ]
    }
   ],
   "source": [
    "lm = skl_lm.LinearRegression()\n",
    "model = lm.fit(Xtrain, Ytrain)\n",
    "#Ytrain = (model.coef_) * Xtrain + (model.intersept_)\n",
    "print(model.coef_,model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the <tt>predict()</tt> function to estimate the response for the test\n",
    "observations, and evaluate it prediction with function <tt>mean_squared_error</tt> in <tt>sklearn.metrics</tt>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.361902892587224\n"
     ]
    }
   ],
   "source": [
    "predic = model.predict(Xtest) #Make prediction\n",
    "#print(predic)\n",
    "#Assess how good the model is\n",
    "MSE = mean_squared_error(Ytest, predic)  #Error from \"true\" value (Ytest)\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Quadratic regression</b>:\n",
    "We use function <tt>PolynomialFeatures</tt> to estimate the test error for the polynomial and cubic regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 97.]\n",
      " [ 75.]\n",
      " [ 75.]\n",
      " [112.]\n",
      " [ 67.]] \n",
      " [[1.0000e+00 9.7000e+01 9.4090e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 7.5000e+01 5.6250e+03]\n",
      " [1.0000e+00 1.1200e+02 1.2544e+04]\n",
      " [1.0000e+00 6.7000e+01 4.4890e+03]]\n",
      "[[165.]\n",
      " [150.]\n",
      " [150.]\n",
      " [215.]\n",
      " [170.]] \n",
      " [[1.0000e+00 1.6500e+02 2.7225e+04]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 1.5000e+02 2.2500e+04]\n",
      " [1.0000e+00 2.1500e+02 4.6225e+04]\n",
      " [1.0000e+00 1.7000e+02 2.8900e+04]]\n"
     ]
    }
   ],
   "source": [
    "# Quadratic polynomial (degree=2)\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "Xtrain2 = poly.fit_transform(Xtrain) #IMPORTANT: Puts each X-value -> 1 / X / X^2\n",
    "Xtest2 = poly.fit_transform(Xtest)\n",
    "print(Xtrain[0:5],'\\n',Xtrain2[0:5])\n",
    "print(Xtest[0:5],'\\n',Xtest2[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.         -0.51860376  0.00141527] 60.30223777216642\n",
      "20.252690858345748\n"
     ]
    }
   ],
   "source": [
    "# Make model\n",
    "quadmodel = lm.fit(Xtrain2, Ytrain)\n",
    "print(quadmodel.coef_,quadmodel.intercept_)\n",
    "quadMSE = mean_squared_error(Ytest, quadmodel.predict(Xtest2))\n",
    "print(quadMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems that quadratic is better.... :-)\n",
    "\n",
    "<b>Cubic regression</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00000000e+00 -6.86758109e-01  2.80449742e-03 -3.52379474e-06] 66.51996119784485\n",
      "20.325609365972525\n"
     ]
    }
   ],
   "source": [
    "# Cubic polynomial\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "\n",
    "# Puts each X-value -> 1 / X / X^2 / X^3\n",
    "Xtrain3 = poly.fit_transform(Xtrain)\n",
    "Xtest3 = poly.fit_transform(Xtest)\n",
    "\n",
    "cubicmodel = lm.fit(Xtrain3, Ytrain)\n",
    "print(cubicmodel.coef_,cubicmodel.intercept_)\n",
    "cubicMSE = mean_squared_error(Ytest, model.predict(Xtest3))\n",
    "print(cubicMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hmm... now got bit worse... shall we continue?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial order 4\n",
      "Coefficients for polynomial order 4\n",
      "[ 0.00000000e+00 -4.07089951e-01 -5.26969867e-04  1.45699704e-05\n",
      " -3.61932229e-08] 57.089007717397536\n",
      "and MSE:\n",
      "19.82328003535277\n",
      "Polynomial order 5\n",
      "Coefficients for polynomial order 5\n",
      "[ 0.00000000e+00  2.57078850e+00 -5.27277434e-02  4.47228844e-04\n",
      " -1.73823447e-06  2.55305510e-09] -7.010477966978165\n",
      "and MSE:\n",
      "19.115946589914845\n",
      "Polynomial order 6\n",
      "Coefficients for polynomial order 6\n",
      "[ 0.00000000e+00  5.98932799e+00 -1.29674595e-01  1.32557738e-03\n",
      " -7.11271127e-06  1.93230131e-08 -2.09263717e-11] -67.12990022730582\n",
      "and MSE:\n",
      "18.891373340709364\n"
     ]
    }
   ],
   "source": [
    "print(\"Polynomial order 4\")\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "Xtrain4 = poly.fit_transform(Xtrain)\n",
    "Xtest4 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain4, Ytrain)\n",
    "MSE = mean_squared_error(Ytest, model.predict(Xtest4))\n",
    "print(\"Coefficients for polynomial order 4\")\n",
    "print(model.coef_,model.intercept_)\n",
    "print(\"and MSE:\")\n",
    "print(MSE)\n",
    "print(\"Polynomial order 5\")\n",
    "poly = PolynomialFeatures(degree=5)\n",
    "Xtrain5 = poly.fit_transform(Xtrain)\n",
    "Xtest5 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain5, Ytrain)\n",
    "MSE = mean_squared_error(Ytest, model.predict(Xtest5))\n",
    "print(\"Coefficients for polynomial order 5\")\n",
    "print(model.coef_,model.intercept_)\n",
    "print(\"and MSE:\")\n",
    "print(MSE)\n",
    "print(\"Polynomial order 6\")\n",
    "poly = PolynomialFeatures(degree=6)\n",
    "Xtrain6 = poly.fit_transform(Xtrain)\n",
    "Xtest6 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain6, Ytrain)\n",
    "MSE = mean_squared_error(Ytest, model.predict(Xtest6))\n",
    "print(\"Coefficients for polynomial order 6\")\n",
    "print(model.coef_,model.intercept_)\n",
    "print(\"and MSE:\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note</b>: These different error rates are due to the choice of the training set. If we choose a different\n",
    "training set instead, then we will obtain somewhat different errors on the\n",
    "validation set. We can test this out by setting a different random seed (different <tt>random_state</tt>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear fit\n",
      "[-0.15097529] 39.0131086591854\n",
      "25.10853905288967\n",
      "Quadratic fit\n",
      "[ 0.         -0.45662488  0.00120242] 56.04672548439809\n",
      "19.722533470492426\n",
      "Cubic fit\n",
      "[ 0.00000000e+00 -6.87826381e-01  3.09360410e-03 -4.74583024e-06] 64.68435751138958\n",
      "19.92136786007267\n",
      "Polynomial order 4\n",
      "Coefficients for polynomial order 4\n",
      "[ 0.00000000e+00 -4.07089951e-01 -5.26969867e-04  1.45699704e-05\n",
      " -3.61932229e-08] 57.089007717397536\n",
      "and MSE:\n",
      "19.82328003535277\n",
      "Polynomial order 5\n",
      "Coefficients for polynomial order 5\n",
      "[ 0.00000000e+00  2.57078850e+00 -5.27277434e-02  4.47228844e-04\n",
      " -1.73823447e-06  2.55305510e-09] -7.010477966978165\n",
      "and MSE:\n",
      "19.115946589914845\n",
      "Polynomial order 6\n",
      "Coefficients for polynomial order 6\n",
      "[ 0.00000000e+00  5.98932799e+00 -1.29674595e-01  1.32557738e-03\n",
      " -7.11271127e-06  1.93230131e-08 -2.09263717e-11] -67.12990022730582\n",
      "and MSE:\n",
      "18.891373340709364\n"
     ]
    }
   ],
   "source": [
    "# Make two other subsets (training and testing)\n",
    "other_train_set = thisdata.sample(196, random_state = 2)\n",
    "other_test_set = thisdata[~thisdata.isin(other_train_set)].dropna(how = 'all')  \n",
    "\n",
    "# \"Extract\" the variables 'horsepower' and 'mpg'\n",
    "Xtrain = other_train_set['horsepower'].values.reshape(-1,1) \n",
    "Ytrain = other_train_set['mpg'].values\n",
    "Xtest = other_test_set['horsepower'].values.reshape(-1,1)\n",
    "Ytest = other_test_set['mpg'].values\n",
    "\n",
    "print(\"Linear fit\")\n",
    "model = lm.fit(Xtrain, Ytrain)\n",
    "print(model.coef_,model.intercept_)\n",
    "print(mean_squared_error(Ytest, model.predict(Xtest)))\n",
    "\n",
    "print(\"Quadratic fit\")\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "Xtrain2 = poly.fit_transform(Xtrain)\n",
    "Xtest2 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain2, Ytrain)\n",
    "print(model.coef_,model.intercept_)\n",
    "print(mean_squared_error(Ytest, model.predict(Xtest2)))\n",
    "\n",
    "print(\"Cubic fit\")\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "Xtrain3 = poly.fit_transform(Xtrain)\n",
    "Xtest3 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain3, Ytrain)\n",
    "print(model.coef_,model.intercept_)\n",
    "print(mean_squared_error(Ytest, model.predict(Xtest3)))\n",
    "\n",
    "print(\"Polynomial order 4\")\n",
    "poly = PolynomialFeatures(degree=4)\n",
    "Xtrain4 = poly.fit_transform(Xtrain)\n",
    "Xtest4 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain4, Ytrain)\n",
    "MSE = mean_squared_error(Ytest, model.predict(Xtest4))\n",
    "print(\"Coefficients for polynomial order 4\")\n",
    "print(model.coef_,model.intercept_)\n",
    "print(\"and MSE:\")\n",
    "print(MSE)\n",
    "\n",
    "print(\"Polynomial order 5\")\n",
    "poly = PolynomialFeatures(degree=5)\n",
    "Xtrain5 = poly.fit_transform(Xtrain)\n",
    "Xtest5 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain5, Ytrain)\n",
    "MSE = mean_squared_error(Ytest, model.predict(Xtest5))\n",
    "print(\"Coefficients for polynomial order 5\")\n",
    "print(model.coef_,model.intercept_)\n",
    "print(\"and MSE:\")\n",
    "print(MSE)\n",
    "\n",
    "print(\"Polynomial order 6\")\n",
    "poly = PolynomialFeatures(degree=6)\n",
    "Xtrain6 = poly.fit_transform(Xtrain)\n",
    "Xtest6 = poly.fit_transform(Xtest)\n",
    "model = lm.fit(Xtrain6, Ytrain)\n",
    "MSE = mean_squared_error(Ytest, model.predict(Xtest6))\n",
    "print(\"Coefficients for polynomial order 6\")\n",
    "print(model.coef_,model.intercept_)\n",
    "print(\"and MSE:\")\n",
    "print(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could use here a <tt>for</tt>-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial order 1\n",
      "Coefficients for polynomial order 1\n",
      "[ 0.         -0.15097529] 39.0131086591854\n",
      "and MSE:\n",
      "25.10853905288967\n",
      "Polynomial order 2\n",
      "Coefficients for polynomial order 2\n",
      "[ 0.         -0.45662488  0.00120242] 56.04672548439809\n",
      "and MSE:\n",
      "19.722533470492426\n",
      "Polynomial order 3\n",
      "Coefficients for polynomial order 3\n",
      "[ 0.00000000e+00 -6.87826381e-01  3.09360410e-03 -4.74583024e-06] 64.68435751138958\n",
      "and MSE:\n",
      "19.92136786007267\n",
      "Polynomial order 4\n",
      "Coefficients for polynomial order 4\n",
      "[ 0.00000000e+00 -4.07089951e-01 -5.26969867e-04  1.45699704e-05\n",
      " -3.61932229e-08] 57.089007717397536\n",
      "and MSE:\n",
      "19.82328003535277\n",
      "Polynomial order 5\n",
      "Coefficients for polynomial order 5\n",
      "[ 0.00000000e+00  2.57078850e+00 -5.27277434e-02  4.47228844e-04\n",
      " -1.73823447e-06  2.55305510e-09] -7.010477966978165\n",
      "and MSE:\n",
      "19.115946589914845\n",
      "Polynomial order 6\n",
      "Coefficients for polynomial order 6\n",
      "[ 0.00000000e+00  5.98932799e+00 -1.29674595e-01  1.32557738e-03\n",
      " -7.11271127e-06  1.93230131e-08 -2.09263717e-11] -67.12990022730582\n",
      "and MSE:\n",
      "18.891373340709364\n"
     ]
    }
   ],
   "source": [
    "for order in range(1,7):\n",
    "    print(\"Polynomial order \"+str(order))\n",
    "    poly = PolynomialFeatures(degree=order)\n",
    "    thisXtrain = poly.fit_transform(Xtrain)\n",
    "    thisXtest = poly.fit_transform(Xtest)\n",
    "    thismodel = lm.fit(thisXtrain, Ytrain)\n",
    "    thisMSE = mean_squared_error(Ytest, thismodel.predict(thisXtest))\n",
    "    print(\"Coefficients for polynomial order \"+str(order))\n",
    "    print(thismodel.coef_,thismodel.intercept_)\n",
    "    print(\"and MSE:\")\n",
    "    print(thisMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this split of the observations into a training set and a validation\n",
    "set, we find that the validation set error rates for the models with linear,\n",
    "quadratic, and cubic terms are 25.11, 19.72, and 19.92, respectively.\n",
    "\n",
    "These results are consistent with our previous findings: a model that\n",
    "predicts ${\\tt mpg}$ using a quadratic function of ${\\tt horsepower}$ performs better than\n",
    "a model that involves only a linear function of ${\\tt horsepower}$, and there is\n",
    "little evidence in favor of a model that uses a cubic function of ${\\tt horsepower}$.\n",
    "\n",
    "### 5.3.2 Leave-One-Out Cross-Validation\n",
    "\n",
    "The LOOCV estimate can be automatically computed for any generalized linear model using the `LeaveOneOut()` and `KFold()` functions. Note: <tt>LeaveOneOut()</tt> is equivalent to <tt>KFold(n_splits=n)</tt> and <tt>LeavePOut(p=1)</tt> where <tt>n</tt> is the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score  #needed for computing the errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets go back to the linear model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lm.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, get the number of splitting iterations in the cross-validator (one-by-one leads to number of data points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "392\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "X = thisdata['horsepower'].values.reshape(-1,1)\n",
    "y = thisdata['mpg'].values.reshape(-1,1)\n",
    "nsplits = loo.get_n_splits(X)\n",
    "print(nsplits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for the next window: The parameter <tt>scoring</tt> designates scorer object with the scoring parameter. All scorer objects follow the convention that higher return values are better than lower return values. Thus metrics which measure the distance between the model and the data, like metrics.mean_squared_error, are available as <i>negative values</i>, thus <tt>neg_mean_squared_error</tt> which return the symmetric value of the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folds: 392, MSE: 24.231513517929226, STD: 36.79731503640535\n"
     ]
    }
   ],
   "source": [
    "crossvalidation = KFold(n_splits=392, random_state=None, shuffle=False)\n",
    "scores = cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=crossvalidation, n_jobs=1)\n",
    "#print(scores)\n",
    "print(\"Folds: \" + str(len(scores)) + \", MSE: \" + str(np.mean(np.abs(scores))) + \", STD: \" + str(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the same for all the other polynomial fits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree-1 polynomial MSE: 24.231513517929226, STD: 36.797315036405344\n",
      "Degree-2 polynomial MSE: 19.248213124489745, STD: 34.998446151782346\n",
      "Degree-3 polynomial MSE: 19.33498406411498, STD: 35.76513567797254\n",
      "Degree-4 polynomial MSE: 19.424430307079398, STD: 35.68335275228356\n",
      "Degree-5 polynomial MSE: 19.033198669299846, STD: 35.31730233206771\n",
      "Degree-6 polynomial MSE: 18.998016005583708, STD: 35.3208471634935\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "    X_current = poly.fit_transform(X)\n",
    "    model = lm.fit(X_current, y)\n",
    "    scores = cross_val_score(model, X_current, y, scoring=\"neg_mean_squared_error\", cv=crossvalidation,\n",
    " n_jobs=1)\n",
    "    print(\"Degree-\"+str(i)+\" polynomial MSE: \" + str(np.mean(np.abs(scores))) + \", STD: \" + str(np.std(scores)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a sharp drop in the estimated test MSE between\n",
    "the linear and quadratic fits, but then no clear improvement from using\n",
    "higher-order polynomials. What should we conclude? (Remember the balance between accuracy and complexity... :-))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.3 k-Fold Cross-Validation\n",
    "\n",
    "Instead of using <tt>n</tt> as the number of \"folds\" in the <tt>KFold</tt> function we can use (preferably!) a small number, say k=10, on the `Auto` data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "thiscrossvalidation = KFold(n_splits=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do polynomial regression for that new kind of cross-validation (<tt>cv=thiscrossvalidation</tt>). Note: notice how faster <tt>Kfold</tt> is in comparison with <tt>LOOCV</tt>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree-1 polynomial MSE: 27.439933652339864, STD: 14.510250711281133\n",
      "Degree-2 polynomial MSE: 21.235840055801567, STD: 11.797327528897465\n",
      "Degree-3 polynomial MSE: 21.336606183382038, STD: 11.844339714535932\n",
      "Degree-4 polynomial MSE: 21.353886969306874, STD: 11.986332290218549\n",
      "Degree-5 polynomial MSE: 20.905558736691837, STD: 12.185388793921343\n",
      "Degree-6 polynomial MSE: 20.780544653507675, STD: 12.135161876520757\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,7):\n",
    "    poly = PolynomialFeatures(degree=i)\n",
    "    X_current = poly.fit_transform(X)\n",
    "    model = lm.fit(X_current, y)\n",
    "    scores = cross_val_score(model, X_current, y, scoring=\"neg_mean_squared_error\", cv=thiscrossvalidation,\n",
    " n_jobs=1)\n",
    "    print(\"Degree-\"+str(i)+\" polynomial MSE: \" + str(np.mean(np.abs(scores))) + \", STD: \" + str(np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still see little evidence that using cubic or higher-order polynomial terms leads to lower test error than simply using a quadratic fit.\n",
    "\n",
    "<b>Can you apply CV to your data set? Try it!</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
